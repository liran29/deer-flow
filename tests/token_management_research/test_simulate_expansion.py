#!/usr/bin/env python3
"""
æ¨¡æ‹ŸçœŸå®çš„ Token æ‰©å±•åœºæ™¯
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from src.utils.token_manager import TokenManager
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.messages.utils import trim_messages
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def create_expansion_scenario():
    """åˆ›å»ºä¸€ä¸ªå¯èƒ½å¯¼è‡´ token æ‰©å±•çš„åœºæ™¯"""
    
    # æ¨¡æ‹Ÿ reporter èŠ‚ç‚¹æ”¶åˆ°çš„æ¶ˆæ¯æ ¼å¼
    messages = [
        SystemMessage(content="Generate comprehensive research report."),
        HumanMessage(content="Research Topic: Framework Comparison"),
        AIMessage(content="I'll analyze the frameworks based on collected data."),
        HumanMessage(content="""
# Existing Research Findings

## Finding 1: Performance Data
TensorFlow: 95% efficiency in distributed training
PyTorch: 87% efficiency, better debugging
JAX: 99% efficiency, limited ecosystem

## Finding 2: Architecture Analysis  
- TensorFlow: Static graph optimization
- PyTorch: Dynamic computation graphs
- JAX: Functional programming approach

## Finding 3: Community Support
TensorFlow: 180k GitHub stars
PyTorch: 150k GitHub stars  
JAX: 25k GitHub stars
        """),
        AIMessage(content="Based on this data, I need to process observations."),
        HumanMessage(content="Include performance benchmarks and user experience analysis.")
    ]
    
    return messages

def test_manual_token_expansion():
    """æ‰‹åŠ¨æµ‹è¯•å¯èƒ½å¯¼è‡´æ‰©å±•çš„æ“ä½œ"""
    
    print("\n" + "="*80)
    print("ğŸ§ª æ‰‹åŠ¨æ¨¡æ‹Ÿ Token æ‰©å±•æµ‹è¯•")
    print("="*80)
    
    messages = create_expansion_scenario()
    token_manager = TokenManager()
    
    print(f"ğŸ“ åŸå§‹æ¶ˆæ¯æ•°: {len(messages)}")
    
    # 1. é¦–å…ˆè®¡ç®—åŸå§‹ token æ•°
    from src.utils.token_counter import TokenCounterFactory
    counter = TokenCounterFactory.create_counter("deepseek-chat")
    
    # Convert to dict format for counting
    orig_dicts = []
    for msg in messages:
        if hasattr(msg, '__class__'):
            msg_type = msg.__class__.__name__
            if msg_type == 'SystemMessage':
                role = 'system'
            elif msg_type == 'HumanMessage':
                role = 'user'
            elif msg_type == 'AIMessage':
                role = 'assistant'
            else:
                role = 'user'
        
        orig_dicts.append({
            'role': role,
            'content': str(msg.content)
        })
    
    orig_tokens = counter.count_messages_tokens(orig_dicts)
    print(f"ğŸ”¢ åŸå§‹ tokens: {orig_tokens:,}")
    
    # 2. ä½¿ç”¨ trim_messages å¤„ç†
    print("\nğŸ”„ ä½¿ç”¨ trim_messages å¤„ç†...")
    
    def test_token_counter(messages_list):
        # Same conversion logic as in TokenManager
        message_dicts = []
        for msg in messages_list:
            if hasattr(msg, 'content'):
                if hasattr(msg, '__class__'):
                    msg_type = msg.__class__.__name__
                    if msg_type == 'SystemMessage':
                        role = 'system'
                    elif msg_type == 'HumanMessage':
                        role = 'user' 
                    elif msg_type == 'AIMessage':
                        role = 'assistant'
                    else:
                        role = 'user'
                else:
                    role = 'user'
                
                message_dicts.append({
                    'role': role,
                    'content': str(msg.content)
                })
        
        return counter.count_messages_tokens(message_dicts)
    
    # Apply trim_messages directly
    trimmed = trim_messages(
        messages=messages,
        max_tokens=12000,  # reporter limit
        strategy="last",
        token_counter=test_token_counter
    )
    
    # Count trimmed tokens
    trimmed_tokens = test_token_counter(trimmed)
    
    print(f"ğŸ“Š å¤„ç†ç»“æœ:")
    print(f"   æ¶ˆæ¯æ•°: {len(messages)} â†’ {len(trimmed)}")
    print(f"   Tokens: {orig_tokens:,} â†’ {trimmed_tokens:,}")
    print(f"   å˜åŒ–: {((trimmed_tokens - orig_tokens) / orig_tokens * 100):+.1f}%")
    
    # 3. ç°åœ¨ç”¨ TokenManager å¤„ç†çœ‹æ˜¯å¦æœ‰å·®å¼‚
    print(f"\nğŸ”„ ä½¿ç”¨ TokenManager å¤„ç†...")
    
    result = token_manager.trim_messages_for_node(
        messages, "deepseek-chat", "reporter"
    )
    
    print(f"ğŸ¯ TokenManager ç»“æœ: {len(messages)} â†’ {len(result)} æ¶ˆæ¯")
    
    # 4. æ£€æŸ¥æ¶ˆæ¯å†…å®¹æ˜¯å¦æœ‰å˜åŒ–
    print(f"\nğŸ” å†…å®¹åˆ†æ:")
    
    # æ¯”è¾ƒç¬¬ä¸€æ¡å’Œæœ€åä¸€æ¡æ¶ˆæ¯
    if messages and result:
        orig_first_len = len(str(messages[0].content))
        trim_first_len = len(str(result[0].content))
        print(f"   ç¬¬ä¸€æ¡æ¶ˆæ¯é•¿åº¦: {orig_first_len} â†’ {trim_first_len}")
        
        if len(messages) > 1 and len(result) > 1:
            orig_last_len = len(str(messages[-1].content))
            trim_last_len = len(str(result[-1].content))
            print(f"   æœ€åä¸€æ¡æ¶ˆæ¯é•¿åº¦: {orig_last_len} â†’ {trim_last_len}")
    
    # 5. æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„å†…å®¹æ·»åŠ 
    orig_content = " ".join(str(msg.content) for msg in messages)
    trim_content = " ".join(str(msg.content) for msg in result)
    
    print(f"   æ€»å†…å®¹é•¿åº¦: {len(orig_content)} â†’ {len(trim_content)}")
    
    if len(trim_content) > len(orig_content):
        print("âš ï¸  æ£€æµ‹åˆ°å†…å®¹å¢åŠ ï¼Œå¯èƒ½çš„åŸå› :")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æ ¼å¼åŒ–
        if "##" in trim_content and "##" not in orig_content:
            print("     - æ·»åŠ äº† Markdown æ ‡é¢˜æ ¼å¼")
        
        if "<finding>" in trim_content and "<finding>" not in orig_content:
            print("     - æ·»åŠ äº†ç»“æ„åŒ–æ ‡ç­¾")
            
        if "observation" in trim_content.lower() and "observation" not in orig_content.lower():
            print("     - æ·»åŠ äº†è§‚å¯Ÿæ•°æ®æ ‡è¯†")

def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸ”¬ Token æ‰©å±•æ·±åº¦åˆ†æå®éªŒ")
    print("=" * 80)
    print("ç›®æ ‡: ç†è§£å¹¶é‡ç° token æ‰©å±•ç°è±¡")
    
    test_manual_token_expansion()
    
    print("\n" + "="*80)
    print("ğŸ“‹ å®éªŒæ€»ç»“")
    print("="*80)
    print("âœ… é€šè¿‡ç›´æ¥å¯¹æ¯” trim_messages çš„è¾“å…¥è¾“å‡º")
    print("âœ… åˆ†æäº†å¯èƒ½å¯¼è‡´æ‰©å±•çš„å…·ä½“åŸå› ")
    print("âœ… å¢å¼ºäº†ç›‘æ§å’Œè¯Šæ–­èƒ½åŠ›")
    print("ğŸ’¡ ä¸‹æ¬¡åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é‡åˆ°æ‰©å±•æ—¶ï¼Œå°†è·å¾—è¯¦ç»†çš„åˆ†ææŠ¥å‘Š")
    print("="*80 + "\n")

if __name__ == "__main__":
    main()