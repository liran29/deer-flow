#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›åçš„ Token ç®¡ç†ç›‘æ§
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from src.utils.token_manager import TokenManager
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import logging

# Configure logging to see all token management activities
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def test_improved_monitoring():
    """æµ‹è¯•æ”¹è¿›åçš„ Token ç®¡ç†ç›‘æ§"""
    
    print("\n" + "="*80)
    print("ğŸ”§ æµ‹è¯•æ”¹è¿›åçš„ Token ç®¡ç†ç›‘æ§")
    print("="*80 + "\n")
    
    token_manager = TokenManager()
    
    # Test Case 1: Normal scenario (should show NO_TRIM)
    print("ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ 1: æ­£å¸¸åœºæ™¯ (å°‘é‡æ¶ˆæ¯)")
    print("-" * 50)
    
    small_messages = [
        SystemMessage(content="You are a helpful assistant."),
        HumanMessage(content="ç®€å•é—®é¢˜"),
        AIMessage(content="ç®€å•å›ç­”")
    ]
    
    result1 = token_manager.trim_messages_for_node(
        small_messages, "deepseek-chat", "planner"
    )
    print(f"ç»“æœ: {len(small_messages)} â†’ {len(result1)} æ¡æ¶ˆæ¯")
    
    # Test Case 2: Large scenario (should show TRIMMED)
    print("\nğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ 2: å¤§é‡æ¶ˆæ¯åœºæ™¯ (åº”è¯¥è£å‰ª)")
    print("-" * 50)
    
    large_messages = [SystemMessage(content="You are a helpful assistant.")]
    
    # Add many large messages
    for i in range(150):
        large_messages.append(HumanMessage(content=f"å¤§é‡é—®é¢˜ {i}: " + "å†…å®¹" * 200))
        large_messages.append(AIMessage(content=f"å¤§é‡å›ç­” {i}: " + "è¯¦ç»†å›ç­”" * 300))
    
    result2 = token_manager.trim_messages_for_node(
        large_messages, "deepseek-chat", "planner"
    )
    print(f"ç»“æœ: {len(large_messages)} â†’ {len(result2)} æ¡æ¶ˆæ¯")
    
    # Test Case 3: Different nodes
    print("\nğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ 3: ä¸åŒèŠ‚ç‚¹çš„é…ç½®")
    print("-" * 50)
    
    test_nodes = ["planner", "reporter", "researcher", "background_investigation"]
    
    for node in test_nodes:
        print(f"\nğŸ” æµ‹è¯•èŠ‚ç‚¹: {node}")
        result = token_manager.trim_messages_for_node(
            large_messages[:50], "deepseek-chat", node  # Use subset for different effects
        )
        strategy = token_manager.get_trimming_strategy(node)
        print(f"   é…ç½®çš„ max_tokens: {strategy.get('max_tokens', 'N/A')}")
        print(f"   å®é™…ç»“æœ: {len(large_messages[:50])} â†’ {len(result)} æ¡æ¶ˆæ¯")
    
    # Test Case 4: Model limits
    print("\nğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ 4: ä¸åŒæ¨¡å‹é™åˆ¶")
    print("-" * 50)
    
    test_models = ["deepseek-chat", "gemini-2.0-flash", "gpt-4"]
    
    for model in test_models:
        limit = token_manager.get_model_limit(model)
        print(f"ğŸ¤– {model}: {limit:,} tokens")
        
        result = token_manager.trim_messages_for_node(
            large_messages[:30], model, "planner"
        )
        print(f"   å¤„ç†ç»“æœ: {len(large_messages[:30])} â†’ {len(result)} æ¡æ¶ˆæ¯")
    
    print("\n" + "="*80)
    print("âœ… Token ç®¡ç†ç›‘æ§æµ‹è¯•å®Œæˆ!")
    print("ğŸ” è¯·æ£€æŸ¥ä¸Šæ–¹çš„è¯¦ç»†ç›‘æ§æ—¥å¿—")
    print("="*80 + "\n")

if __name__ == "__main__":
    test_improved_monitoring()