#!/usr/bin/env python3
"""
ç®€å•æµ‹è¯• token ç®¡ç†ç³»ç»Ÿ
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from src.config.configuration import Configuration
from src.utils.token_manager import TokenManager
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import logging

# Configure logging to see token management logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def test_token_management():
    """æµ‹è¯• token ç®¡ç†åŠŸèƒ½"""
    
    print("\n" + "="*80)
    print("ğŸ§ª Token ç®¡ç†ç³»ç»Ÿç®€å•æµ‹è¯•")
    print("="*80 + "\n")
    
    # Initialize token manager
    token_manager = TokenManager()
    
    # Check configuration
    print(f"âœ… Token ç®¡ç†ç³»ç»Ÿå·²åˆå§‹åŒ–")
    print(f"ğŸ“Š æµ‹è¯•æ¨¡å‹: deepseek-chat (32K tokens)")
    
    # Create test messages
    messages = [
        SystemMessage(content="You are a helpful AI assistant."),
        HumanMessage(content="Hello, I need help with a complex research task."),
        AIMessage(content="I'll help you with your research. What would you like to know?"),
    ]
    
    # Add many messages to simulate a long conversation
    print("\nğŸ”„ ç”Ÿæˆå¤§é‡æµ‹è¯•æ¶ˆæ¯...")
    for i in range(100):
        messages.append(HumanMessage(content=f"Question {i}: " + "x" * 500))
        messages.append(AIMessage(content=f"Answer {i}: " + "y" * 1000))
    
    # Test trimming for different nodes
    test_cases = [
        ("planner", "deepseek-chat"),
        ("reporter", "deepseek-chat"),
        ("background_investigation", "deepseek-chat"),
    ]
    
    print(f"\nğŸ“‹ åŸå§‹æ¶ˆæ¯æ•°é‡: {len(messages)}")
    
    for node_name, model_name in test_cases:
        print(f"\n{'='*60}")
        print(f"ğŸ” æµ‹è¯•èŠ‚ç‚¹: {node_name}")
        print(f"ğŸ¤– æ¨¡å‹: {model_name}")
        print(f"{'='*60}")
        
        # Apply token management
        trimmed = token_manager.trim_messages_for_node(
            messages, model_name, node_name
        )
        
        print(f"âœ… æ¶ˆæ¯è£å‰ªå®Œæˆ!")
        print(f"   - è£å‰ªåæ¶ˆæ¯æ•°: {len(trimmed)}")
        
        # Check if extreme trimming happened
        if len(trimmed) == 0:
            print(f"âš ï¸  è­¦å‘Š: æ‰€æœ‰æ¶ˆæ¯éƒ½è¢«è£å‰ªäº†!")
        elif len(trimmed) == 1:
            print(f"âš ï¸  æ³¨æ„: åªä¿ç•™äº†ç³»ç»Ÿæ¶ˆæ¯")
        
    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆ! æ£€æŸ¥ä¸Šé¢çš„æ—¥å¿—äº†è§£è¯¦ç»†çš„ token ä½¿ç”¨æƒ…å†µ")
    print("="*80 + "\n")

if __name__ == "__main__":
    test_token_management()