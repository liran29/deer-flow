#!/usr/bin/env python3
"""
Demo script to show token management in action.

This script demonstrates how the token management system works by:
1. Creating large inputs that exceed model token limits
2. Showing how different models handle different token loads
3. Demonstrating the logging output from token management
"""

import sys
import os
import logging
import asyncio

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../..'))
sys.path.insert(0, project_root)

# Configure logging to show token management details
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

from src.utils.token_manager import TokenManager
from src.utils.token_counter import count_tokens
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage


def demo_basic_token_management():
    """Demonstrate basic token management functionality"""
    print("=== Token Management Demo ===\n")
    
    token_manager = TokenManager()
    
    # Create a large input that would exceed DeepSeek's limit
    large_content = """
    Âú£ËØûË£ÖÈ•∞ÂìÅÂ∏ÇÂú∫Ê≠£Âú®ÁªèÂéÜÂâçÊâÄÊú™ÊúâÁöÑÂèòÈù©„ÄÇÊô∫ËÉΩÂåñË£ÖÈ•∞ÂìÅÊàê‰∏∫Êñ∞Ë∂ãÂäøÔºåLEDÊô∫ËÉΩÂΩ©ÁÅØ„ÄÅÂèØÁºñÁ®ãË£ÖÈ•∞ÂìÅ„ÄÅ
    ËØ≠Èü≥ÊéßÂà∂Ë£ÖÈ•∞Á≠âÁßëÊäÄÂÖÉÁ¥†ÂèóÂà∞Ê∂àË¥πËÄÖÁÉ≠ÁÉàÊ¨¢Ëøé„ÄÇÁéØ‰øùÊùêÊñôÂ∫îÁî®‰πüÊòØÈáçË¶ÅË∂ãÂäøÔºåÂèØÂõûÊî∂Â°ëÊñô„ÄÅ
    Â§©ÁÑ∂ÊùêÊñôÂà∂ÊàêÁöÑÂú£ËØûÁî®ÂìÅÈúÄÊ±ÇÊåÅÁª≠Â¢ûÈïø„ÄÇ‰∏™ÊÄßÂåñÂÆöÂà∂ÊúçÂä°Êàê‰∏∫Â∑ÆÂºÇÂåñÁ´û‰∫âÂÖ≥ÈîÆÔºå
    Ê∂àË¥πËÄÖÂØπÁã¨ÁâπÂÆöÂà∂Âåñ‰∫ßÂìÅÈúÄÊ±Ç‰∏çÊñ≠Â¢ûÈïø„ÄÇ
    """ * 500  # Repeat to make it large
    
    print(f"Large content length: {len(large_content):,} characters")
    
    # Count tokens for different models
    deepseek_tokens = count_tokens(large_content, "deepseek-chat")
    gemini_tokens = count_tokens(large_content, "gemini-2.0-flash")
    
    print(f"DeepSeek tokens: {deepseek_tokens:,} (limit: 32,768)")
    print(f"Gemini tokens: {gemini_tokens:,} (limit: 1,000,000)")
    print(f"DeepSeek exceeds limit by: {max(0, deepseek_tokens - 32768):,} tokens\n")
    
    # Create messages that include this large content
    messages = [
        SystemMessage(content="‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑÂ∏ÇÂú∫ÂàÜÊûêÂ∏àÔºåËØ∑ËØ¶ÁªÜÂàÜÊûêÊèê‰æõÁöÑ‰ø°ÊÅØ„ÄÇ"),
        HumanMessage(content=f"ËØ∑ÂàÜÊûê‰ª•‰∏ãÂ∏ÇÂú∫‰ø°ÊÅØÔºö\n\n{large_content}"),
        HumanMessage(content="ËØ∑Êèê‰æõÂÖ∑‰ΩìÁöÑÊï∞ÊçÆÂàÜÊûêÂíåÂ∏ÇÂú∫È¢ÑÊµã„ÄÇ")
    ]
    
    print(f"Original messages: {len(messages)}")
    
    # Apply token management for DeepSeek
    print("\n--- DeepSeek Token Management ---")
    trimmed_deepseek = token_manager.trim_messages_for_node(messages, "deepseek-chat", "planner")
    print(f"Trimmed messages: {len(trimmed_deepseek)}")
    
    # Apply token management for Gemini
    print("\n--- Gemini Token Management ---")
    trimmed_gemini = token_manager.trim_messages_for_node(messages, "gemini-2.0-flash", "planner")
    print(f"Trimmed messages: {len(trimmed_gemini)}")
    
    # Show the difference
    trimmed_deepseek_content = " ".join([msg.content for msg in trimmed_deepseek if hasattr(msg, 'content')])
    trimmed_gemini_content = " ".join([msg.content for msg in trimmed_gemini if hasattr(msg, 'content')])
    
    deepseek_final_tokens = count_tokens(trimmed_deepseek_content, "deepseek-chat")
    gemini_final_tokens = count_tokens(trimmed_gemini_content, "gemini-2.0-flash")
    
    print(f"\nFinal token counts:")
    print(f"DeepSeek: {deepseek_final_tokens:,} tokens ({(deepseek_final_tokens/deepseek_tokens*100):.1f}% of original)")
    print(f"Gemini: {gemini_final_tokens:,} tokens ({(gemini_final_tokens/gemini_tokens*100):.1f}% of original)")


def demo_observation_management():
    """Demonstrate observation management"""
    print("\n=== Observation Management Demo ===\n")
    
    token_manager = TokenManager()
    
    # Create many detailed observations
    observations = []
    for i in range(15):
        observation = f"""
        Á†îÁ©∂ËßÇÂØü {i}: ÈÄöËøáÂØπÂú£ËØûË£ÖÈ•∞ÂìÅÂ∏ÇÂú∫ÁöÑÊ∑±ÂÖ•Ë∞ÉÁ†îÂèëÁé∞ÔºåÊ∂àË¥πËÄÖË°å‰∏∫Ê≠£Âú®ÂèëÁîüÊòæËëóÂèòÂåñ„ÄÇ
        Êô∫ËÉΩÂåñ‰∫ßÂìÅÁöÑÊé•ÂèóÂ∫¶Â§ßÂπÖÊèêÂçáÔºåÁâπÂà´ÊòØÂπ¥ËΩªÊ∂àË¥πÁæ§‰ΩìÂØπÁßëÊäÄÂÖÉÁ¥†ÁöÑËøΩÊ±ÇÊòéÊòæ„ÄÇ
        ‰ª∑Ê†ºÊïèÊÑüÂ∫¶ÊúâÊâÄ‰∏ãÈôçÔºåÊ∂àË¥πËÄÖÊõ¥ÊÑøÊÑè‰∏∫È´òË¥®Èáè„ÄÅ‰∏™ÊÄßÂåñÁöÑ‰∫ßÂìÅÊîØ‰ªòÊ∫¢‰ª∑„ÄÇ
        Á∫ø‰∏äË¥≠‰π∞Ê∏†ÈÅìÂç†ÊØîÊåÅÁª≠Â¢ûÈïøÔºåÁ§æ‰∫§Â™í‰ΩìÂΩ±ÂìçË¥≠‰π∞ÂÜ≥Á≠ñÁöÑ‰ΩúÁî®Êó•ÁõäÊòæËëó„ÄÇ
        ÁéØ‰øùÊÑèËØÜÁöÑÊèêÂçáÊé®Âä®‰∫ÜÂèØÊåÅÁª≠‰∫ßÂìÅÁöÑÈúÄÊ±ÇÂ¢ûÈïøÔºåËøô‰∏∫Ë°å‰∏öËΩ¨ÂûãÊèê‰æõ‰∫ÜÊñ∞Êú∫ÈÅá„ÄÇ
        """ * 20  # Make each observation quite long
        observations.append(observation.strip())
    
    print(f"Original observations: {len(observations)}")
    print(f"Average length: {sum(len(obs) for obs in observations) / len(observations):.0f} characters")
    
    # Apply observation management
    managed_observations = token_manager.manage_observations(observations)
    
    print(f"Managed observations: {len(managed_observations)}")
    print(f"Average length: {sum(len(obs) for obs in managed_observations) / len(managed_observations):.0f} characters")
    
    total_original = sum(len(obs) for obs in observations)
    total_managed = sum(len(obs) for obs in managed_observations)
    reduction = (1 - total_managed / total_original) * 100
    
    print(f"Content reduction: {reduction:.1f}%")


def demo_real_world_scenario():
    """Demonstrate a real-world scenario"""
    print("\n=== Real-World Scenario Demo ===\n")
    print("Simulating a workflow that would fail without token management...\n")
    
    # This simulates what would happen in the actual deer-flow workflow
    # when processing large background investigation results
    
    # Large background investigation result (simulated)
    background_result = """
    Âú£ËØûË£ÖÈ•∞ÂìÅÂÖ®ÁêÉÂ∏ÇÂú∫Ë∞ÉÁ†îÊä•ÂëäÔºö
    
    1. Â∏ÇÂú∫ËßÑÊ®°ÂàÜÊûê
    ÂÖ®ÁêÉÂú£ËØûË£ÖÈ•∞ÂìÅÂ∏ÇÂú∫ËßÑÊ®°È¢ÑËÆ°2025Âπ¥Â∞ÜËææÂà∞180‰∫øÁæéÂÖÉÔºåËæÉ2024Âπ¥Â¢ûÈïø8.5%„ÄÇ
    ‰∏ªË¶ÅÂ¢ûÈïøÈ©±Âä®Âõ†Á¥†ÂåÖÊã¨ÔºöÊ∂àË¥πÂçáÁ∫ß„ÄÅÁßëÊäÄÂàõÊñ∞„ÄÅ‰∏™ÊÄßÂåñÈúÄÊ±ÇÂ¢ûÈïø„ÄÇ
    
    2. Âú∞Âå∫ÂàÜÂ∏É
    ÂåóÁæéÂ∏ÇÂú∫Âç†ÊØî35%ÔºåÊ¨ßÊ¥≤Â∏ÇÂú∫Âç†ÊØî30%Ôºå‰∫öÂ§™Â∏ÇÂú∫Âç†ÊØî25%ÔºåÂÖ∂‰ªñÂú∞Âå∫Âç†ÊØî10%„ÄÇ
    ‰∫öÂ§™Â∏ÇÂú∫Â¢ûÈïøÊúÄ‰∏∫ËøÖÈÄüÔºåÂπ¥Â§çÂêàÂ¢ûÈïøÁéáËææÂà∞12.3%„ÄÇ
    
    3. ‰∫ßÂìÅÁ±ªÂà´ÂàÜÊûê
    LEDÂΩ©ÁÅØÁ±ª‰∫ßÂìÅÂç†ÊØî40%ÔºåË£ÖÈ•∞ÊëÜ‰ª∂Âç†ÊØî25%ÔºåÂú£ËØûÊ†ëÂèäÈÖç‰ª∂Âç†ÊØî20%ÔºåÂÖ∂‰ªñÂç†ÊØî15%„ÄÇ
    Êô∫ËÉΩÂåñ‰∫ßÂìÅÂ¢ûÈïøÊòæËëóÔºåÈ¢ÑËÆ°Âπ¥Â¢ûÈïøÁéáË∂ÖËøá20%„ÄÇ
    
    4. Ê∂àË¥πËÄÖË°å‰∏∫ÂèòÂåñ
    Á∫ø‰∏äË¥≠‰π∞ÊØî‰æã‰ªé2020Âπ¥ÁöÑ30%Â¢ûÈïøÂà∞2025Âπ¥È¢ÑÊúüÁöÑ65%„ÄÇ
    Ê∂àË¥πËÄÖÊõ¥Ê≥®Èáç‰∫ßÂìÅÁöÑÁéØ‰øùÂ±ûÊÄßÂíå‰∏™ÊÄßÂåñÁâπÂæÅ„ÄÇ
    ‰ª∑Ê†ºÊïèÊÑüÂ∫¶‰∏ãÈôçÔºåÂìÅË¥®Âíå‰ΩìÈ™åÊàê‰∏∫‰∏ªË¶ÅËÄÉÈáèÂõ†Á¥†„ÄÇ
    
    5. ÊäÄÊúØÂàõÊñ∞Ë∂ãÂäø
    Áâ©ËÅîÁΩëÊäÄÊúØÂ∫îÁî®ÔºöÊô∫ËÉΩÊéßÂà∂„ÄÅËøúÁ®ãÊìç‰Ωú„ÄÅÂú∫ÊôØËÅîÂä®
    Êñ∞ÊùêÊñôÂ∫îÁî®ÔºöÁîüÁâ©ÈôçËß£ÊùêÊñô„ÄÅÂõûÊî∂ÊùêÊñô„ÄÅËäÇËÉΩÊùêÊñô
    ËÆæËÆ°ÂàõÊñ∞Ôºö3DÊâìÂç∞ÂÆöÂà∂„ÄÅAR/VR‰ΩìÈ™å„ÄÅAIÊé®Ëçê
    
    6. Á´û‰∫âÊ†ºÂ±Ä
    ‰º†ÁªüÂà∂ÈÄ†ÂïÜÂä†ÈÄüÊï∞Â≠óÂåñËΩ¨ÂûãÔºåÁßëÊäÄÂÖ¨Âè∏Ë∑®ÁïåËøõÂÖ•Â∏ÇÂú∫„ÄÇ
    ÂìÅÁâåÈõÜ‰∏≠Â∫¶ÊèêÂçáÔºåÂ§¥ÈÉ®‰ºÅ‰∏öÂ∏ÇÂú∫‰ªΩÈ¢ùÊâ©Â§ß„ÄÇ
    ‰æõÂ∫îÈìæÂÖ®ÁêÉÂåñË∂ãÂäøÊòéÊòæÔºåÂå∫ÂüüÂåñÁîü‰∫ßÂ¢ûÂä†„ÄÇ
    
    7. ÊåëÊàò‰∏éÊú∫ÈÅá
    ÊåëÊàòÔºöÂéüÊùêÊñô‰ª∑Ê†ºÊ≥¢Âä®„ÄÅÁéØ‰øùÊ≥ïËßÑË∂ã‰∏•„ÄÅÊ∂àË¥πÈúÄÊ±ÇÂ§öÊ†∑Âåñ
    Êú∫ÈÅáÔºöÊñ∞ÂÖ¥Â∏ÇÂú∫Â¢ûÈïø„ÄÅÊäÄÊúØÂàõÊñ∞È©±Âä®„ÄÅÂèØÊåÅÁª≠ÂèëÂ±ïÈúÄÊ±Ç
    """ * 100  # Make it very large
    
    token_manager = TokenManager()
    
    # Check if this would exceed limits
    bg_tokens = count_tokens(background_result, "deepseek-chat")
    print(f"Background investigation result: {bg_tokens:,} tokens")
    print(f"DeepSeek limit: 32,768 tokens")
    print(f"Exceeds limit by: {max(0, bg_tokens - 32768):,} tokens")
    
    # Show how the system would handle this
    strategy = token_manager.get_trimming_strategy("background_investigation")
    max_length = strategy.get("max_tokens", 2000) * 4
    
    if len(background_result) > max_length:
        trimmed_bg = background_result[:max_length] + "\n\n[... results truncated for token management ...]"
        print(f"\nBackground investigation would be trimmed:")
        print(f"Original: {len(background_result):,} characters")
        print(f"Trimmed: {len(trimmed_bg):,} characters")
        print(f"Reduction: {((len(background_result) - len(trimmed_bg)) / len(background_result) * 100):.1f}%")
    
    print(f"\n‚úì Token management prevents workflow failures!")
    print(f"‚úì Large inputs are automatically handled!")
    print(f"‚úì Different models get appropriate token allocations!")


if __name__ == "__main__":
    print("ü¶å Deer-Flow Token Management Demo")
    print("=" * 50)
    
    try:
        demo_basic_token_management()
        demo_observation_management()
        demo_real_world_scenario()
        
        print("\n" + "=" * 50)
        print("‚úì Token management is working correctly!")
        print("‚úì Your deer-flow system can now handle models like DeepSeek!")
        print("‚úì Check the logs above to see token reduction in action!")
        
    except Exception as e:
        print(f"\n‚ùå Demo failed: {e}")
        import traceback
        traceback.print_exc()