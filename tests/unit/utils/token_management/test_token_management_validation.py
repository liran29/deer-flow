#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆToken ManagementéªŒè¯è„šæœ¬

ä¸“æ³¨äºéªŒè¯tokenç®¡ç†æ ¸å¿ƒåŠŸèƒ½ï¼Œé¿å…å¤æ‚çš„nodeé›†æˆæµ‹è¯•ã€‚
"""

import sys
import os
import logging
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def create_test_content():
    """Create test content of various sizes."""
    base_content = """
    å…¨çƒAIèŠ¯ç‰‡äº§ä¸šæ·±åº¦åˆ†ææŠ¥å‘Šï¼šæŠ€æœ¯é©æ–°ä¸å¸‚åœºç«äº‰æ ¼å±€ç ”ç©¶
    
    ä¸€ã€äº§ä¸šæ¦‚è¿°ä¸å‘å±•ç°çŠ¶
    äººå·¥æ™ºèƒ½èŠ¯ç‰‡ä½œä¸ºé©±åŠ¨AIæŠ€æœ¯è½åœ°çš„æ ¸å¿ƒç¡¬ä»¶åŸºç¡€è®¾æ–½ï¼Œæ­£åœ¨ç»å†å‰æ‰€æœªæœ‰çš„å¿«é€Ÿå‘å±•æœŸã€‚
    ä»å¸‚åœºè§„æ¨¡æ¥çœ‹ï¼Œå…¨çƒAIèŠ¯ç‰‡å¸‚åœºå·²ä»2020å¹´çš„130äº¿ç¾å…ƒå¢é•¿åˆ°2024å¹´çš„è¶…è¿‡500äº¿ç¾å…ƒï¼Œ
    é¢„è®¡åˆ°2030å¹´å°†çªç ´2000äº¿ç¾å…ƒå¤§å…³ï¼Œå¹´å¤åˆå¢é•¿ç‡ä¿æŒåœ¨25%ä»¥ä¸Šçš„é«˜ä½æ°´å¹³ã€‚
    
    æŠ€æœ¯æ¶æ„æ–¹é¢ï¼ŒAIèŠ¯ç‰‡æ­£åœ¨ä»ä¼ ç»Ÿçš„é€šç”¨è®¡ç®—æ¶æ„å‘ä¸“ç”¨AIè®¡ç®—æ¶æ„æ¼”è¿›ã€‚
    ç¥ç»ç½‘ç»œå¤„ç†å™¨(NPU)ã€å›¾å½¢å¤„ç†å™¨(GPU)ã€ç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—(FPGA)ç­‰ä¸åŒæ¶æ„
    åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹å±•ç°å‡ºå„è‡ªçš„ä¼˜åŠ¿ç‰¹ç‚¹ã€‚å­˜ç®—ä¸€ä½“åŒ–ã€å…‰å­è®¡ç®—ã€é‡å­è®¡ç®—ç­‰
    å‰æ²¿æŠ€æœ¯ä¹Ÿåœ¨é€æ­¥ä»å®éªŒå®¤èµ°å‘å•†ä¸šåŒ–åº”ç”¨ã€‚
    
    äºŒã€æŠ€æœ¯å‘å±•è¶‹åŠ¿åˆ†æ
    1. è®¡ç®—æ¶æ„åˆ›æ–°ï¼šä¸“ç”¨åŒ–ç¨‹åº¦ä¸æ–­æå‡
    - ä»CPU/GPUé€šç”¨è®¡ç®—å‘NPUä¸“ç”¨è®¡ç®—æ¼”è¿›
    - å­˜ç®—ä¸€ä½“åŒ–æ¶æ„çªç ´å†¯Â·è¯ºä¾æ›¼ç“¶é¢ˆé™åˆ¶
    - å¯é‡æ„è®¡ç®—æä¾›çµæ´»æ€§ä¸æ€§èƒ½çš„å¹³è¡¡
    - ç±»è„‘è®¡ç®—æ¶æ„æ¨¡æ‹Ÿäººè„‘ç¥ç»ç½‘ç»œç»“æ„
    
    2. åˆ¶ç¨‹å·¥è‰ºç«äº‰ï¼šå…ˆè¿›åˆ¶ç¨‹æˆä¸ºå…³é”®
    - 5çº³ç±³åˆ¶ç¨‹æŠ€æœ¯å·²å®ç°å¤§è§„æ¨¡é‡äº§
    - 3çº³ç±³åˆ¶ç¨‹å³å°†è¿›å…¥å•†ä¸šåŒ–é˜¶æ®µ
    - 2.5D/3Då°è£…æŠ€æœ¯çªç ´æ€§èƒ½ç“¶é¢ˆ
    - æ–°ææ–™åº”ç”¨æ¨åŠ¨å™¨ä»¶æ€§èƒ½æå‡
    
    3. è½¯ä»¶ç”Ÿæ€å»ºè®¾ï¼šå¼€å‘å·¥å…·é“¾æ—¥è¶‹å®Œå–„
    - ç¼–è¯‘å™¨ä¼˜åŒ–æå‡ä»£ç æ‰§è¡Œæ•ˆç‡
    - å¼€å‘æ¡†æ¶ç®€åŒ–AIåº”ç”¨å¼€å‘æµç¨‹
    - è°ƒè¯•å·¥å…·å¢å¼ºç³»ç»Ÿå¯è§‚æµ‹æ€§
    - æ€§èƒ½åˆ†æå·¥å…·ä¼˜åŒ–èµ„æºåˆ©ç”¨
    """ * 50  # é‡å¤50æ¬¡åˆ›å»ºå¤§å†…å®¹
    
    return {
        'small': base_content,
        'medium': base_content * 5,
        'large': base_content * 20,
        'massive': base_content * 100
    }

def test_token_counter_accuracy():
    """æµ‹è¯•tokenè®¡æ•°å™¨çš„å‡†ç¡®æ€§"""
    logger.info("ğŸ§ª Testing Token Counter Accuracy...")
    
    try:
        from src.utils.token_counter import TokenCounterFactory, count_tokens
        
        test_content = create_test_content()
        
        # æµ‹è¯•ä¸åŒæ¨¡å‹çš„tokenè®¡æ•°
        models = ["deepseek-chat", "gemini-2.0-flash", "gpt-4"]
        
        for model in models:
            logger.info(f"\n--- Testing {model} ---")
            counter = TokenCounterFactory.create_counter(model)
            
            for size, content in test_content.items():
                token_count = count_tokens(content, model)
                char_count = len(content)
                ratio = char_count / token_count if token_count > 0 else 0
                
                logger.info(f"{size:>8} content: {char_count:>8,} chars = {token_count:>6,} tokens (ratio: {ratio:.1f})")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Token counter test failed: {e}")
        return False

def test_token_trimming_effectiveness():
    """æµ‹è¯•tokenä¿®å‰ªçš„æœ‰æ•ˆæ€§"""
    logger.info("\nğŸ§ª Testing Token Trimming Effectiveness...")
    
    try:
        from src.utils.token_manager import TokenManager
        from src.utils.token_counter import count_tokens
        from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
        
        token_manager = TokenManager()
        test_content = create_test_content()
        
        # æµ‹è¯•ä¸åŒè§„æ¨¡å†…å®¹çš„ä¿®å‰ªæ•ˆæœ
        for size_name, content in test_content.items():
            logger.info(f"\n--- Testing {size_name} content ---")
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§ä¸šåˆ†æå¸ˆã€‚"),
                HumanMessage(content=f"è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š{content}"),
                AIMessage(content="æˆ‘å°†ä¸ºæ‚¨æä¾›è¯¦ç»†çš„åˆ†æã€‚"),
                HumanMessage(content="è¯·æä¾›å…·ä½“çš„æ•°æ®åˆ†æå’Œå»ºè®®ã€‚")
            ]
            
            # è®¡ç®—åŸå§‹tokenæ•°
            original_tokens = sum(count_tokens(msg.content, "deepseek-chat") for msg in messages)
            
            # åº”ç”¨tokenç®¡ç†
            trimmed_messages = token_manager.trim_messages_for_node(messages, "deepseek-chat", "planner")
            trimmed_tokens = sum(count_tokens(msg.content, "deepseek-chat") for msg in trimmed_messages)
            
            # è®¡ç®—æ•ˆæœ
            reduction = ((original_tokens - trimmed_tokens) / original_tokens * 100) if original_tokens > 0 else 0
            
            logger.info(f"Original: {original_tokens:>6,} tokens | Trimmed: {trimmed_tokens:>6,} tokens | Reduction: {reduction:>5.1f}%")
            
            # éªŒè¯æ˜¯å¦åœ¨é™åˆ¶èŒƒå›´å†…
            deepseek_limit = 32768
            if trimmed_tokens <= deepseek_limit:
                logger.info(f"âœ… Within DeepSeek limit ({deepseek_limit:,} tokens)")
            else:
                logger.warning(f"âš ï¸  Exceeds DeepSeek limit ({deepseek_limit:,} tokens)")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Token trimming test failed: {e}")
        return False

def test_different_model_limits():
    """æµ‹è¯•ä¸åŒæ¨¡å‹é™åˆ¶çš„å¤„ç†"""
    logger.info("\nğŸ§ª Testing Different Model Limits...")
    
    try:
        from src.utils.token_manager import TokenManager
        from src.utils.token_counter import count_tokens
        from langchain_core.messages import HumanMessage
        
        token_manager = TokenManager()
        large_content = create_test_content()['massive']
        
        message = HumanMessage(content=large_content)
        
        # æµ‹è¯•ä¸åŒæ¨¡å‹çš„å¤„ç†
        models = [
            ("deepseek-chat", 32768),
            ("gemini-2.0-flash", 1000000), 
            ("gpt-4", 128000)
        ]
        
        for model, expected_limit in models:
            logger.info(f"\n--- Testing {model} (limit: {expected_limit:,}) ---")
            
            original_tokens = count_tokens(message.content, model)
            trimmed_messages = token_manager.trim_messages_for_node([message], model, "planner")
            trimmed_tokens = sum(count_tokens(msg.content, model) for msg in trimmed_messages)
            
            model_limit = token_manager.get_model_limit(model)
            
            logger.info(f"Original tokens: {original_tokens:>8,}")
            logger.info(f"Trimmed tokens:  {trimmed_tokens:>8,}")
            logger.info(f"Model limit:     {model_limit:>8,}")
            logger.info(f"Within limit: {'âœ… Yes' if trimmed_tokens <= model_limit else 'âŒ No'}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Model limits test failed: {e}")
        return False

def test_observation_management():
    """æµ‹è¯•è§‚å¯Ÿç®¡ç†åŠŸèƒ½"""
    logger.info("\nğŸ§ª Testing Observation Management...")
    
    try:
        from src.utils.token_manager import TokenManager
        
        token_manager = TokenManager()
        
        # åˆ›å»ºå¤§é‡é•¿è§‚å¯Ÿ
        observations = []
        for i in range(25):
            long_obs = f"è§‚å¯Ÿ{i}: " + "è¿™æ˜¯ä¸€ä¸ªéå¸¸è¯¦ç»†çš„è§‚å¯Ÿç»“æœï¼ŒåŒ…å«å¤§é‡æ•°æ®å’Œåˆ†æã€‚" * 300
            observations.append(long_obs)
        
        logger.info(f"Original observations: {len(observations)}")
        total_chars = sum(len(obs) for obs in observations)
        logger.info(f"Total characters: {total_chars:,}")
        
        # åº”ç”¨è§‚å¯Ÿç®¡ç†
        managed_observations = token_manager.manage_observations(observations)
        
        logger.info(f"Managed observations: {len(managed_observations)}")
        managed_chars = sum(len(obs) for obs in managed_observations)
        logger.info(f"Managed characters: {managed_chars:,}")
        
        reduction = ((total_chars - managed_chars) / total_chars * 100) if total_chars > 0 else 0
        logger.info(f"Character reduction: {reduction:.1f}%")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Observation management test failed: {e}")
        return False

def test_configuration_loading():
    """æµ‹è¯•é…ç½®åŠ è½½åŠŸèƒ½"""
    logger.info("\nğŸ§ª Testing Configuration Loading...")
    
    try:
        from src.utils.token_manager import TokenManager
        
        token_manager = TokenManager()
        
        # æµ‹è¯•é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½
        logger.info(f"Config path: {token_manager.config_path}")
        logger.info(f"Config exists: {token_manager.config_path.exists()}")
        logger.info(f"Token management enabled: {token_manager.token_management.get('enabled', False)}")
        
        # æµ‹è¯•æ¨¡å‹é™åˆ¶
        test_models = ["deepseek-chat", "gemini-2.0-flash", "gpt-4", "unknown-model"]
        for model in test_models:
            limit = token_manager.get_model_limit(model)
            logger.info(f"{model}: {limit:,} tokens")
        
        # æµ‹è¯•ä¿®å‰ªç­–ç•¥
        test_nodes = ["planner", "reporter", "background_investigation", "unknown_node"]
        for node in test_nodes:
            strategy = token_manager.get_trimming_strategy(node)
            max_tokens = strategy.get("max_tokens", "Not set")
            logger.info(f"{node}: {max_tokens} max tokens")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Configuration test failed: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•"""
    logger.info("ğŸ¦Œ Starting Simplified Token Management Validation")
    logger.info("=" * 70)
    
    tests = [
        ("Configuration Loading", test_configuration_loading),
        ("Token Counter Accuracy", test_token_counter_accuracy),
        ("Token Trimming Effectiveness", test_token_trimming_effectiveness),
        ("Different Model Limits", test_different_model_limits),
        ("Observation Management", test_observation_management),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ” Running: {test_name}")
        logger.info(f"{'='*70}")
        
        try:
            results[test_name] = test_func()
        except Exception as e:
            logger.error(f"âŒ {test_name} crashed: {e}")
            results[test_name] = False
    
    # æ€»ç»“
    logger.info(f"\n{'='*70}")
    logger.info("ğŸ“Š VALIDATION RESULTS SUMMARY")
    logger.info(f"{'='*70}")
    
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… PASSED" if result else "âŒ FAILED"
        logger.info(f"{status} - {test_name}")
    
    logger.info(f"\nğŸ¯ Overall Result: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("ğŸ‰ ALL TOKEN MANAGEMENT VALIDATIONS PASSED!")
        logger.info("ğŸš€ Your deer-flow system is ready for production with small-limit models!")
        logger.info("ğŸ’¡ Token management successfully prevents token overflow errors!")
    else:
        logger.info("âš ï¸  Some validations failed. Review the logs above for details.")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)