#!/usr/bin/env python3
"""
æµ‹è¯•å¸¦æœ‰å¤§é‡å†å²æ¶ˆæ¯çš„å·¥ä½œæµ
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from src.graph import build_graph
from src.graph.types import State
from langchain_core.messages import HumanMessage, AIMessage
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

async def test_workflow_with_history():
    """æµ‹è¯•å¸¦æœ‰å¤§é‡å†å²æ¶ˆæ¯çš„å·¥ä½œæµ"""
    
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•å¸¦å†å²æ¶ˆæ¯çš„ Token ç®¡ç†")
    print("="*80 + "\n")
    
    # Build graph
    graph = build_graph()
    
    # Create initial state with many messages
    messages = []
    
    # Add many historical messages to trigger token management
    print("ğŸ“ æ„å»ºå¤§é‡å†å²æ¶ˆæ¯...")
    for i in range(100):
        messages.append(HumanMessage(content=f"å†å²é—®é¢˜ {i}: " + "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„é—®é¢˜" * 50))
        messages.append(AIMessage(content=f"å†å²å›ç­” {i}: " + "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„å›ç­”" * 100))
    
    # Add the actual query
    messages.append(HumanMessage(content="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"))
    
    print(f"ğŸ“Š æ¶ˆæ¯æ€»æ•°: {len(messages)} æ¡")
    print(f"ğŸ’¾ ä¼°è®¡ tokens: ~{sum(len(msg.content) for msg in messages) // 4:,}")
    
    initial_state = {
        "messages": messages,
        "next": "coordinator",
        "plan": [],
        "plan_step_results": {},
        "plan_steps_taken": 0,
    }
    
    config = {
        "configurable": {
            "locale": "zh-CN",
            "researcher": {
                "llm_type": "basic",
                "max_concurrent_searches": 3,
                "search_engines": ["tavily", "crawl"],
                "extra_tools": {
                    "add_to_agents": ["researcher"],
                }
            }
        },
        "recursion_limit": 50,
    }
    
    print("\nğŸš€ å¯åŠ¨å·¥ä½œæµ...\n")
    print("ğŸ’¡ æ³¨æ„è§‚å¯Ÿ Token Management æ—¥å¿—\n")
    
    try:
        # Run the workflow
        async for s in graph.astream(
            input=initial_state, 
            config=config, 
            stream_mode="values"
        ):
            # Only print key messages
            if isinstance(s, dict) and "messages" in s:
                last_msg = s["messages"][-1] if s["messages"] else None
                if last_msg and hasattr(last_msg, 'name'):
                    print(f"âœ“ {last_msg.name} èŠ‚ç‚¹å·²æ‰§è¡Œ")
                    
        print("\nâœ… å·¥ä½œæµå®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*80)
    print("ğŸ’¡ æ£€æŸ¥ä¸Šæ–¹æ—¥å¿—ä¸­çš„ Token Management è¾“å‡º")
    print("="*80 + "\n")

async def main():
    """ä¸»å‡½æ•°"""
    await test_workflow_with_history()

if __name__ == "__main__":
    asyncio.run(main())