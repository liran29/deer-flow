#!/usr/bin/env python3
"""
Token Management Validation Script

This script tests the token management system in real deer-flow workflows
to ensure it properly handles large inputs that would exceed model token limits.
"""

import sys
import os
import logging
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Configure logging to see token management in action
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def create_large_background_content():
    """Create background content that definitely exceeds token limits."""
    base_content = """
    æ·±åº¦å¸‚åœºåˆ†ææŠ¥å‘Šï¼šå…¨çƒAIèŠ¯ç‰‡äº§ä¸šå‘å±•è¶‹åŠ¿ä¸ç«äº‰æ ¼å±€åˆ†æ
    
    ä¸€ã€äº§ä¸šæ¦‚è¿°
    äººå·¥æ™ºèƒ½èŠ¯ç‰‡ä½œä¸ºAIæŠ€æœ¯è½åœ°çš„æ ¸å¿ƒç¡¬ä»¶æ”¯æ’‘ï¼Œè¿‘å¹´æ¥å¸‚åœºéœ€æ±‚çˆ†å‘å¼å¢é•¿ã€‚
    å…¨çƒAIèŠ¯ç‰‡å¸‚åœºè§„æ¨¡ä»2020å¹´çš„130äº¿ç¾å…ƒå¢é•¿åˆ°2024å¹´çš„è¶…è¿‡500äº¿ç¾å…ƒï¼Œ
    é¢„è®¡åˆ°2030å¹´å°†è¾¾åˆ°2000äº¿ç¾å…ƒï¼Œå¹´å¤åˆå¢é•¿ç‡è¶…è¿‡25%ã€‚
    
    äºŒã€æŠ€æœ¯å‘å±•è¶‹åŠ¿
    1. æ¶æ„åˆ›æ–°ï¼šä»ä¼ ç»ŸCPU/GPUåˆ°ä¸“ç”¨AIåŠ é€Ÿå™¨
    - ç¥ç»ç½‘ç»œå¤„ç†å™¨(NPU)æˆä¸ºä¸»æµ
    - å¯é‡æ„è®¡ç®—æ¶æ„å…´èµ·
    - å­˜ç®—ä¸€ä½“åŒ–æŠ€æœ¯çªç ´
    - å…‰å­è®¡ç®—å¼€å§‹å•†ä¸šåŒ–æ¢ç´¢
    
    2. åˆ¶ç¨‹å·¥è‰ºï¼šå…ˆè¿›åˆ¶ç¨‹ç«äº‰æ¿€çƒˆ
    - 5nmåˆ¶ç¨‹é‡äº§ï¼Œ3nmåˆ¶ç¨‹å³å°†æ™®åŠ
    - å…ˆè¿›å°è£…æŠ€æœ¯åˆ›æ–°ï¼Œå¦‚2.5Dã€3Då°è£…
    - æ–°ææ–™åº”ç”¨ï¼Œå¦‚åŒ–åˆç‰©åŠå¯¼ä½“
    - é‡å­è®¡ç®—èŠ¯ç‰‡æŠ€æœ¯çªç ´
    
    3. åº”ç”¨åœºæ™¯å¤šæ ·åŒ–
    - äº‘ç«¯è®­ç»ƒï¼šå¤§æ¨¡å‹è®­ç»ƒéœ€æ±‚æš´å¢
    - è¾¹ç¼˜æ¨ç†ï¼šç§»åŠ¨è®¾å¤‡AIè®¡ç®—èƒ½åŠ›æå‡
    - è‡ªåŠ¨é©¾é©¶ï¼šè½¦è½½AIèŠ¯ç‰‡å¸‚åœºå¿«é€Ÿå¢é•¿
    - æ•°æ®ä¸­å¿ƒï¼šAIæœåŠ¡å™¨æ¸—é€ç‡æŒç»­æå‡
    
    ä¸‰ã€ç«äº‰æ ¼å±€åˆ†æ
    1. å…¨çƒå¤´éƒ¨å‚å•†
    - NVIDIAï¼šåœ¨AIè®­ç»ƒèŠ¯ç‰‡é¢†åŸŸå æ®ç»å¯¹é¢†å…ˆåœ°ä½ï¼ŒH100ã€A100ç³»åˆ—äº§å“éœ€æ±‚ç«çˆ†
    - AMDï¼šé€šè¿‡æ”¶è´­Xilinxå¢å¼ºFPGAå®åŠ›ï¼ŒMIç³»åˆ—GPUå¯¹æ ‡NVIDIA
    - Intelï¼šæ”¶è´­Habana Labsè¿›å†›AIè®­ç»ƒï¼Œæ¨å‡ºPonte Vecchioæ¶æ„
    - Googleï¼šTPUç³»åˆ—åœ¨è‡ªå®¶äº‘æœåŠ¡ä¸­å¤§è§„æ¨¡éƒ¨ç½²
    - Appleï¼šMç³»åˆ—èŠ¯ç‰‡é›†æˆNeural Engineï¼Œåœ¨ç§»åŠ¨AIé¢†åŸŸåˆ›æ–°
    
    2. ä¸­å›½å‚å•†å´›èµ·
    - åä¸ºï¼šæ˜‡è…¾ç³»åˆ—AIèŠ¯ç‰‡æŠ€æœ¯ä¸æ–­çªç ´ï¼Œç”Ÿæ€é€æ­¥å®Œå–„
    - å¯’æ­¦çºªï¼šæ€å…ƒç³»åˆ—è¦†ç›–äº‘è¾¹ç«¯å…¨åœºæ™¯
    - åœ°å¹³çº¿ï¼šåœ¨æ±½è½¦æ™ºèƒ½èŠ¯ç‰‡é¢†åŸŸå æ®é‡è¦åœ°ä½
    - æ¯”ç‰¹å¤§é™†ï¼šä»æŒ–çŸ¿èŠ¯ç‰‡è½¬å‘AIæ¨ç†èŠ¯ç‰‡
    - ç‡§åŸç§‘æŠ€ï¼šä¸“æ³¨äºAIè®­ç»ƒèŠ¯ç‰‡ï¼Œé‚ƒæ€ç³»åˆ—æ€§èƒ½ä¼˜å¼‚
    
    å››ã€å¸‚åœºç»†åˆ†
    1. è®­ç»ƒèŠ¯ç‰‡å¸‚åœº
    - å¤§æ¨¡å‹è®­ç»ƒéœ€æ±‚æ¨åŠ¨é«˜ç«¯èŠ¯ç‰‡éœ€æ±‚
    - ç®—åŠ›å¯†åº¦å’Œèƒ½æ•ˆæ¯”æˆä¸ºå…³é”®æŒ‡æ ‡
    - å¤šèŠ¯ç‰‡äº’è”æŠ€æœ¯é‡è¦æ€§å‡¸æ˜¾
    - è½¯ä»¶ç”Ÿæ€å’Œå¼€å‘å·¥å…·é“¾å†³å®šç«äº‰åŠ›
    
    2. æ¨ç†èŠ¯ç‰‡å¸‚åœº
    - è¾¹ç¼˜è®¡ç®—éœ€æ±‚æ¨åŠ¨ä½åŠŸè€—èŠ¯ç‰‡å‘å±•
    - å®æ—¶æ€§è¦æ±‚æ¨åŠ¨ä¸“ç”¨æ¶æ„åˆ›æ–°
    - æˆæœ¬æ•æ„Ÿæ€§ä¿ƒè¿›èŠ¯ç‰‡æ ‡å‡†åŒ–
    - é‡äº§è§„æ¨¡å†³å®šå¸‚åœºåœ°ä½
    
    äº”ã€äº§ä¸šé“¾åˆ†æ
    1. ä¸Šæ¸¸ï¼šè®¾è®¡å·¥å…·å’ŒIP
    - EDAå·¥å…·è¢«å›½å¤–å‚å•†å„æ–­ï¼Œæˆä¸ºäº§ä¸šå‘å±•ç“¶é¢ˆ
    - IPæ ¸æˆæƒè´¹ç”¨æŒç»­ä¸Šæ¶¨
    - å¼€æºRISC-Væ¶æ„æä¾›æ–°é€‰æ‹©
    
    2. ä¸­æ¸¸ï¼šèŠ¯ç‰‡è®¾è®¡ä¸åˆ¶é€ 
    - è®¾è®¡èƒ½åŠ›åˆ†åŒ–åŠ å‰§
    - å…ˆè¿›åˆ¶ç¨‹äº§èƒ½ç¨€ç¼º
    - å°è£…æµ‹è¯•ç¯èŠ‚ç«äº‰æ¿€çƒˆ
    
    3. ä¸‹æ¸¸ï¼šç³»ç»Ÿé›†æˆä¸åº”ç”¨
    - äº‘æœåŠ¡å•†æˆä¸ºé‡è¦å®¢æˆ·
    - OEMå‚å•†éœ€æ±‚å¤šæ ·åŒ–
    - å‚ç›´è¡Œä¸šåº”ç”¨å¿«é€Ÿå¢é•¿
    
    å…­ã€æŠ•èµ„æœºä¼šåˆ†æ
    1. é•¿æœŸçœ‹å¥½é¢†åŸŸ
    - å¤§æ¨¡å‹è®­ç»ƒåŸºç¡€è®¾æ–½
    - è¾¹ç¼˜AIèŠ¯ç‰‡
    - æ±½è½¦æ™ºèƒ½åŒ–èŠ¯ç‰‡
    - æœºå™¨äººæ§åˆ¶èŠ¯ç‰‡
    
    2. å…³é”®æŠ€æœ¯æ–¹å‘
    - å­˜ç®—ä¸€ä½“åŒ–æ¶æ„
    - å…‰ç”µèåˆè®¡ç®—
    - é‡å­è®¡ç®—èŠ¯ç‰‡
    - è„‘æœºæ¥å£èŠ¯ç‰‡
    
    ä¸ƒã€é£é™©å› ç´ 
    1. æŠ€æœ¯é£é™©
    - æ‘©å°”å®šå¾‹æ”¾ç¼“
    - æ–°æ¶æ„ä¸ç¡®å®šæ€§
    - è‰¯ç‡æ§åˆ¶éš¾åº¦
    
    2. å¸‚åœºé£é™©
    - éœ€æ±‚æ³¢åŠ¨æ€§å¤§
    - ç«äº‰åŠ å‰§
    - æ›¿ä»£æŠ€æœ¯å¨èƒ
    
    3. æ”¿ç­–é£é™©
    - è´¸æ˜“é™åˆ¶
    - æŠ€æœ¯å‡ºå£ç®¡åˆ¶
    - äº§ä¸šæ”¿ç­–å˜åŒ–
    
    å…«ã€å‘å±•å»ºè®®
    1. å¯¹ä¼ä¸š
    - åŠ å¼ºæŠ€æœ¯åˆ›æ–°æŠ•å…¥
    - å®Œå–„äº§ä¸šç”Ÿæ€å»ºè®¾
    - é‡è§†äººæ‰åŸ¹å…»
    - æ·±åŒ–å›½é™…åˆä½œ
    
    2. å¯¹æŠ•èµ„è€…
    - å…³æ³¨æŠ€æœ¯é¢†å…ˆä¼ä¸š
    - é‡è§†ç”Ÿæ€å»ºè®¾èƒ½åŠ›
    - è€ƒè™‘äº§ä¸šé“¾å®Œæ•´æ€§
    - è¯„ä¼°å¯æŒç»­å‘å±•èƒ½åŠ›
    """
    
    # Repeat to create massive content (should exceed 32K tokens for DeepSeek)
    return base_content * 100

def create_large_observations():
    """Create large observations list that exceeds memory limits."""
    observations = []
    
    for i in range(20):
        obs = f"""
        è°ƒç ”è§‚å¯Ÿ {i+1}: 
        
        é€šè¿‡å¯¹å…¨çƒAIèŠ¯ç‰‡å¸‚åœºçš„æ·±å…¥è°ƒç ”ï¼Œå‘ç°äº†ä»¥ä¸‹é‡è¦å¸‚åœºåŠ¨æ€å’ŒæŠ€æœ¯è¶‹åŠ¿ï¼š
        
        æŠ€æœ¯å‘å±•æ–¹é¢ï¼š
        - æ–°ä¸€ä»£7nmåˆ¶ç¨‹AIèŠ¯ç‰‡å¼€å§‹é‡äº§ï¼Œç›¸æ¯”ä¸Šä¸€ä»£äº§å“æ€§èƒ½æå‡40%ï¼Œèƒ½è€—é™ä½30%
        - å­˜ç®—ä¸€ä½“åŒ–æ¶æ„å–å¾—é‡å¤§çªç ´ï¼Œå†…å­˜å¸¦å®½åˆ©ç”¨ç‡æå‡è‡³ä¼ ç»Ÿæ¶æ„çš„5å€
        - å¤šæ¨¡æ€AIèŠ¯ç‰‡æˆä¸ºæ–°çƒ­ç‚¹ï¼Œæ”¯æŒè§†è§‰ã€è¯­è¨€ã€éŸ³é¢‘ç­‰å¤šç§AIä»»åŠ¡çš„ç»Ÿä¸€å¤„ç†
        - å…‰å­è®¡ç®—æŠ€æœ¯åœ¨éƒ¨åˆ†åœºæ™¯ä¸‹å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œè®¡ç®—é€Ÿåº¦ç†è®ºä¸Šå¯æå‡1000å€
        
        å¸‚åœºç«äº‰æ ¼å±€ï¼š
        - NVIDIAç»§ç»­åœ¨é«˜ç«¯AIè®­ç»ƒèŠ¯ç‰‡å¸‚åœºä¿æŒé¢†å…ˆï¼Œä½†åœ¨æ¨ç†èŠ¯ç‰‡å¸‚åœºé¢ä¸´æ›´å¤šæŒ‘æˆ˜
        - ä¸­å›½æœ¬åœŸAIèŠ¯ç‰‡å‚å•†åœ¨ç‰¹å®šç»†åˆ†é¢†åŸŸå¼€å§‹å±•ç°ç«äº‰ä¼˜åŠ¿
        - äº‘æœåŠ¡å•†è‡ªç ”èŠ¯ç‰‡è¶‹åŠ¿æ˜æ˜¾ï¼Œäºšé©¬é€Šã€è°·æ­Œã€é˜¿é‡Œäº‘ç­‰éƒ½åœ¨åŠ å¤§æŠ•å…¥
        - æ±½è½¦æ™ºèƒ½åŒ–æ¨åŠ¨è½¦è½½AIèŠ¯ç‰‡éœ€æ±‚çˆ†å‘ï¼Œé¢„è®¡æœªæ¥5å¹´å¸‚åœºè§„æ¨¡å¢é•¿10å€
        
        ä¾›åº”é“¾åŠ¨æ€ï¼š
        - å…ˆè¿›åˆ¶ç¨‹äº§èƒ½ä¾ç„¶ç´§å¼ ï¼Œå°ç§¯ç”µ5nmäº§çº¿æ’æœŸå·²åˆ°2025å¹´
        - å°è£…ææ–™æˆæœ¬ä¸Šæ¶¨ï¼Œæ¨åŠ¨èŠ¯ç‰‡å‚å•†å¯»æ±‚æ–°çš„å°è£…è§£å†³æ–¹æ¡ˆ
        - åœ°ç¼˜æ”¿æ²»å› ç´ å¯¹äº§ä¸šé“¾ç¨³å®šæ€§å½±å“åŠ å¤§ï¼Œä¼ä¸šå¼€å§‹å¸ƒå±€å¤šå…ƒåŒ–ä¾›åº”ç­–ç•¥
        
        åº”ç”¨åœºæ™¯æ‹“å±•ï¼š
        - å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒéœ€æ±‚æ¨åŠ¨è¶…å¤§è§„æ¨¡AIé›†ç¾¤å»ºè®¾
        - è¾¹ç¼˜è®¡ç®—åœºæ™¯ä¸‹çš„AIèŠ¯ç‰‡éœ€æ±‚å¿«é€Ÿå¢é•¿
        - å·¥ä¸šç‰©è”ç½‘ã€æ™ºæ…§åŸå¸‚ç­‰æ–°å…´åº”ç”¨ä¸ºAIèŠ¯ç‰‡æä¾›æ–°çš„å¢é•¿ç‚¹
        - ç§‘å­¦è®¡ç®—ã€è¯ç‰©å‘ç°ç­‰é¢†åŸŸå¼€å§‹å¤§è§„æ¨¡é‡‡ç”¨AIåŠ é€Ÿå™¨
        
        æŠ•èµ„çƒ­ç‚¹åˆ†æï¼š
        - ç§å‹Ÿè‚¡æƒæŠ•èµ„é‡ç‚¹å…³æ³¨åˆåˆ›AIèŠ¯ç‰‡å…¬å¸
        - ä¸Šå¸‚å…¬å¸é€šè¿‡å¹¶è´­æ•´åˆäº§ä¸šé“¾èµ„æº
        - æ”¿åºœäº§ä¸šåŸºé‡‘åŠ å¤§å¯¹æœ¬åœŸAIèŠ¯ç‰‡ä¼ä¸šçš„æ”¯æŒåŠ›åº¦
        - å›½é™…èµ„æœ¬å¯¹ä¸­å›½AIèŠ¯ç‰‡ä¼ä¸šçš„æŠ•èµ„è¶‹äºè°¨æ…
        
        æŠ€æœ¯æ ‡å‡†åŒ–è¿›å±•ï¼š
        - IEEEã€ISOç­‰å›½é™…ç»„ç»‡æ¨åŠ¨AIèŠ¯ç‰‡æ ‡å‡†åˆ¶å®š
        - å¼€æºç¡¬ä»¶æ¶æ„RISC-Våœ¨AIèŠ¯ç‰‡é¢†åŸŸåº”ç”¨å¢å¤š
        - è½¯ä»¶æ¡†æ¶æ ‡å‡†åŒ–æœ‰åŠ©äºé™ä½å¼€å‘æˆæœ¬
        - å®‰å…¨æ ‡å‡†å’Œéšç§ä¿æŠ¤è¦æ±‚æ—¥ç›Šä¸¥æ ¼
        
        æœªæ¥å‘å±•è¶‹åŠ¿é¢„æµ‹ï¼š
        - ä¸“ç”¨AIèŠ¯ç‰‡å°†åœ¨æ›´å¤šå‚ç›´é¢†åŸŸå‡ºç°
        - é‡å­è®¡ç®—ä¸ç»å…¸è®¡ç®—çš„æ··åˆæ¶æ„æˆä¸ºç ”ç©¶çƒ­ç‚¹
        - ç”Ÿç‰©å¯å‘çš„ç¥ç»å½¢æ€èŠ¯ç‰‡æŠ€æœ¯é€æ­¥æˆç†Ÿ
        - å¯é‡æ„è®¡ç®—æ¶æ„ä¸ºAIèŠ¯ç‰‡æä¾›æ›´å¤§çµæ´»æ€§
        """ * 50  # Make each observation very long
        
        observations.append(obs.strip())
    
    return observations

def test_planner_node_integration():
    """Test planner node with large input that exceeds token limits."""
    logger.info("ğŸ§ª Testing Planner Node Token Management...")
    
    try:
        from src.graph.nodes import planner_node
        from src.graph.types import State
        from langchain_core.messages import HumanMessage
        from langchain_core.runnables import RunnableConfig
        
        # Create large background content
        large_content = create_large_background_content()
        logger.info(f"Created background content: {len(large_content):,} characters")
        
        # Create state with large message history
        messages = [
            HumanMessage(content="è¯·åˆ†æå…¨çƒAIèŠ¯ç‰‡äº§ä¸šçš„å‘å±•è¶‹åŠ¿"),
            HumanMessage(content=f"èƒŒæ™¯ä¿¡æ¯ï¼š{large_content}"),
            HumanMessage(content="è¯·æä¾›è¯¦ç»†çš„å¸‚åœºåˆ†æå’ŒæŠ•èµ„å»ºè®®")
        ]
        
        state = State(
            messages=messages,
            research_topic="AIèŠ¯ç‰‡äº§ä¸šåˆ†æ"
        )
        
        # Create proper RunnableConfig
        config = RunnableConfig(
            configurable={
                "max_plan_iterations": 1,
                "max_step_num": 3,
                "max_search_results": 3
            }
        )
        
        logger.info(f"Testing planner with {len(messages)} messages...")
        
        # This should trigger token management
        result = planner_node(state, config)
        
        logger.info("âœ… Planner node completed successfully with token management")
        logger.info(f"Result plan length: {len(str(result.get('plan', '')))}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Planner node test failed: {e}")
        return False

def test_reporter_node_integration():
    """Test reporter node with large observations."""
    logger.info("ğŸ§ª Testing Reporter Node Token Management...")
    
    try:
        from src.graph.nodes import reporter_node
        from src.graph.types import State
        from langchain_core.messages import HumanMessage
        from langchain_core.runnables import RunnableConfig
        
        # Create large observations
        large_observations = create_large_observations()
        logger.info(f"Created {len(large_observations)} observations")
        
        state = State(
            messages=[HumanMessage(content="è¯·æ ¹æ®è°ƒç ”ç»“æœç”ŸæˆæŠ¥å‘Š")],
            research_topic="AIèŠ¯ç‰‡äº§ä¸šåˆ†æ",
            observations=large_observations
        )
        
        # Create proper RunnableConfig
        config = RunnableConfig(
            configurable={
                "max_plan_iterations": 1,
                "max_step_num": 3,
                "max_search_results": 3,
                "report_style": "academic"
            }
        )
        
        logger.info("Testing reporter with large observations...")
        
        # This should trigger observation management
        result = reporter_node(state, config)
        
        logger.info("âœ… Reporter node completed successfully with token management")
        logger.info(f"Generated report length: {len(str(result.get('final_report', '')))}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Reporter node test failed: {e}")
        return False

def test_background_investigation_integration():
    """Test background investigation with large result."""
    logger.info("ğŸ§ª Testing Background Investigation Token Management...")
    
    try:
        from src.graph.nodes import background_investigation_node
        from src.graph.types import State
        from langchain_core.messages import HumanMessage
        from langchain_core.runnables import RunnableConfig
        
        # Create state for background investigation
        state = State(
            messages=[HumanMessage(content="è¯·è°ƒç ”å…¨çƒAIèŠ¯ç‰‡äº§ä¸šçš„æœ€æ–°å‘å±•æƒ…å†µ")],
            research_topic="AIèŠ¯ç‰‡äº§ä¸šå‘å±•è°ƒç ”",
            enable_background_investigation=True
        )
        
        # Create proper RunnableConfig
        config = RunnableConfig(
            configurable={
                "max_plan_iterations": 1,
                "max_step_num": 3,
                "max_search_results": 3
            }
        )
        
        logger.info("Testing background investigation...")
        
        # This should trigger token management if the investigation result is large
        result = background_investigation_node(state, config)
        
        logger.info("âœ… Background investigation completed successfully")
        logger.info(f"Investigation result length: {len(str(result.get('background_investigation_results', '')))}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Background investigation test failed: {e}")
        return False

def test_token_manager_directly():
    """Test TokenManager directly with large inputs."""
    logger.info("ğŸ§ª Testing TokenManager Directly...")
    
    try:
        from src.utils.token_manager import TokenManager
        from src.utils.token_counter import count_tokens
        from langchain_core.messages import HumanMessage, SystemMessage
        
        token_manager = TokenManager()
        
        # Create large content
        large_content = create_large_background_content()
        
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§ä¸šåˆ†æå¸ˆ"),
            HumanMessage(content=f"è¯·åˆ†æä»¥ä¸‹ä¿¡æ¯ï¼š{large_content}"),
            HumanMessage(content="è¯·æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Š")
        ]
        
        # Test with DeepSeek model
        original_tokens = sum(count_tokens(msg.content, "deepseek-chat") for msg in messages)
        logger.info(f"Original content tokens: {original_tokens:,}")
        
        # Apply token management
        trimmed_messages = token_manager.trim_messages_for_node(messages, "deepseek-chat", "planner")
        trimmed_tokens = sum(count_tokens(msg.content, "deepseek-chat") for msg in trimmed_messages)
        
        logger.info(f"Trimmed content tokens: {trimmed_tokens:,}")
        logger.info(f"Token reduction: {((original_tokens - trimmed_tokens) / original_tokens * 100):.1f}%")
        
        # Verify it's within DeepSeek limits
        deepseek_limit = 32768
        if trimmed_tokens <= deepseek_limit:
            logger.info("âœ… Token management successfully kept content within DeepSeek limits")
            return True
        else:
            logger.error(f"âŒ Token management failed: {trimmed_tokens} > {deepseek_limit}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Direct TokenManager test failed: {e}")
        return False

def main():
    """Run all validation tests."""
    logger.info("ğŸ¦Œ Starting Token Management Validation Tests")
    logger.info("=" * 60)
    
    tests = [
        ("Direct TokenManager Test", test_token_manager_directly),
        ("Planner Node Integration", test_planner_node_integration),
        ("Reporter Node Integration", test_reporter_node_integration),
        ("Background Investigation Integration", test_background_investigation_integration),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        logger.info(f"\nğŸ” Running: {test_name}")
        logger.info("-" * 40)
        
        try:
            results[test_name] = test_func()
        except Exception as e:
            logger.error(f"âŒ {test_name} crashed: {e}")
            results[test_name] = False
    
    # Summary
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š VALIDATION RESULTS SUMMARY")
    logger.info("=" * 60)
    
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… PASSED" if result else "âŒ FAILED"
        logger.info(f"{status} - {test_name}")
    
    logger.info(f"\nğŸ¯ Overall: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("ğŸ‰ All token management validations PASSED!")
        logger.info("ğŸš€ Your deer-flow system is ready for production with DeepSeek!")
    else:
        logger.info("âš ï¸  Some validations failed. Review the logs above.")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)