# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
æ™ºèƒ½å†…å®¹å¤„ç†å™¨ - ä¸ºResearcheræä¾›å¤šå±‚æ¬¡å†…å®¹ä¼˜åŒ–
æ ¸å¿ƒç†å¿µ: åˆ†å±‚æå–ã€æ™ºèƒ½å‹ç¼©ã€è¯­ä¹‰ä¿ç•™
"""

import logging
import re
import hashlib
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)


@dataclass
class ContentLayer:
    """å†…å®¹å±‚çº§å®šä¹‰"""
    layer_type: str  # 'summary', 'keywords', 'full'
    content: str
    token_estimate: int
    importance_score: float  # 0-1, é‡è¦æ€§è¯„åˆ†


class ContentProcessor:
    """æ™ºèƒ½å†…å®¹å¤„ç†å™¨"""
    
    def __init__(self, max_tokens_per_content: int = 1000):
        self.max_tokens_per_content = max_tokens_per_content
        self.content_cache: Dict[str, List[ContentLayer]] = {}
        
    def process_crawled_content(
        self, 
        url: str, 
        raw_content: str,
        available_tokens: int = None
    ) -> str:
        """
        æ™ºèƒ½å¤„ç†çˆ¬å–å†…å®¹
        
        ç­–ç•¥:
        1. ç”Ÿæˆå†…å®¹æŒ‡çº¹è¿›è¡Œç¼“å­˜
        2. å¤šå±‚æ¬¡æå–: æ‘˜è¦ -> å…³é”®è¯ -> å®Œæ•´å†…å®¹
        3. æ ¹æ®å¯ç”¨tokenåŠ¨æ€é€‰æ‹©å±‚çº§
        """
        content_hash = self._generate_content_hash(url, raw_content)
        
        # æ£€æŸ¥ç¼“å­˜
        if content_hash in self.content_cache:
            return self._select_optimal_layer(
                self.content_cache[content_hash], 
                available_tokens
            )
        
        # ç”Ÿæˆå¤šå±‚æ¬¡å†…å®¹
        layers = self._generate_content_layers(raw_content)
        self.content_cache[content_hash] = layers
        
        return self._select_optimal_layer(layers, available_tokens)
    
    def _generate_content_hash(self, url: str, content: str) -> str:
        """ç”Ÿæˆå†…å®¹æŒ‡çº¹"""
        return hashlib.md5(f"{url}:{content[:100]}".encode()).hexdigest()
    
    def _generate_content_layers(self, content: str) -> List[ContentLayer]:
        """ç”Ÿæˆå¤šå±‚æ¬¡å†…å®¹"""
        layers = []
        
        # Layer 1: è¶…ç®€æ‘˜è¦ (50-100 tokens)
        summary = self._extract_summary(content, max_length=200)
        layers.append(ContentLayer(
            layer_type="ultra_summary",
            content=summary,
            token_estimate=len(summary) // 4,  # ç²—ç•¥ä¼°ç®—
            importance_score=0.9
        ))
        
        # Layer 2: å…³é”®ä¿¡æ¯ (200-300 tokens)
        keywords = self._extract_key_information(content, max_length=800)
        layers.append(ContentLayer(
            layer_type="key_info",
            content=keywords,
            token_estimate=len(keywords) // 4,
            importance_score=0.8
        ))
        
        # Layer 3: ç»“æ„åŒ–æ‘˜è¦ (500-800 tokens)
        structured = self._extract_structured_content(content, max_length=2000)
        layers.append(ContentLayer(
            layer_type="structured",
            content=structured,
            token_estimate=len(structured) // 4,
            importance_score=0.7
        ))
        
        # Layer 4: å®Œæ•´å†…å®¹ (é™åˆ¶åœ¨max_tokenså†…)
        truncated_full = content[:self.max_tokens_per_content * 4]
        layers.append(ContentLayer(
            layer_type="full",
            content=truncated_full,
            token_estimate=len(truncated_full) // 4,
            importance_score=0.6
        ))
        
        return layers
    
    def _select_optimal_layer(
        self, 
        layers: List[ContentLayer], 
        available_tokens: Optional[int]
    ) -> str:
        """æ ¹æ®å¯ç”¨tokené€‰æ‹©æœ€ä¼˜å±‚çº§"""
        if available_tokens is None:
            available_tokens = self.max_tokens_per_content
        
        # é€‰æ‹©æœ€è¯¦ç»†ä½†ä¸è¶…è¿‡é™åˆ¶çš„å±‚çº§
        for layer in reversed(layers):  # ä»æœ€è¯¦ç»†å¼€å§‹
            if layer.token_estimate <= available_tokens:
                logger.debug(f"Selected {layer.layer_type} layer with {layer.token_estimate} tokens")
                return layer.content
        
        # å¦‚æœéƒ½è¶…é™ï¼Œè¿”å›æœ€ç®€æ‘˜è¦
        return layers[0].content
    
    def _extract_summary(self, content: str, max_length: int) -> str:
        """æå–è¶…ç®€æ‘˜è¦"""
        # ç®€å•ç­–ç•¥: å–å‰å‡ æ®µå¹¶å‹ç¼©
        paragraphs = content.split('\n\n')[:3]
        summary = ' '.join(paragraphs)
        
        if len(summary) > max_length:
            summary = summary[:max_length] + "..."
        
        return f"ğŸ“ **é¡µé¢æ‘˜è¦**: {summary}"
    
    def _extract_key_information(self, content: str, max_length: int) -> str:
        """æå–å…³é”®ä¿¡æ¯"""
        # æå–æ ‡é¢˜ã€åˆ—è¡¨ã€é‡è¦æ•°æ®
        key_info = []
        
        # æå–æ ‡é¢˜
        titles = re.findall(r'^#{1,3}\s+(.+)$', content, re.MULTILINE)
        if titles:
            key_info.append(f"ğŸ·ï¸ **ä¸»è¦ç« èŠ‚**: {', '.join(titles[:5])}")
        
        # æå–æ•°å­—å’Œç»Ÿè®¡
        numbers = re.findall(r'\b\d+(?:,\d{3})*(?:\.\d+)?\s*(?:%|ä¸‡|äº¿|billion|million)\b', content)
        if numbers:
            key_info.append(f"ğŸ“Š **å…³é”®æ•°æ®**: {', '.join(numbers[:10])}")
        
        # æå–åˆ—è¡¨é¡¹
        lists = re.findall(r'^[-*+]\s+(.+)$', content, re.MULTILINE)
        if lists:
            key_info.append(f"ğŸ“‹ **è¦ç‚¹**: {', '.join(lists[:5])}")
        
        result = '\n'.join(key_info)
        if len(result) > max_length:
            result = result[:max_length] + "..."
        
        return result
    
    def _extract_structured_content(self, content: str, max_length: int) -> str:
        """æå–ç»“æ„åŒ–å†…å®¹"""
        sections = []
        
        # æŒ‰æ®µè½åˆ†æ
        paragraphs = content.split('\n\n')
        for i, para in enumerate(paragraphs[:10]):  # æœ€å¤šå¤„ç†10æ®µ
            if len(para.strip()) > 50:  # å¿½ç•¥å¤ªçŸ­çš„æ®µè½
                # å‹ç¼©æ®µè½
                compressed = self._compress_paragraph(para)
                sections.append(f"**ç¬¬{i+1}æ®µ**: {compressed}")
        
        result = '\n\n'.join(sections)
        if len(result) > max_length:
            result = result[:max_length] + "..."
        
        return result
    
    def _compress_paragraph(self, paragraph: str) -> str:
        """å‹ç¼©æ®µè½"""
        # ç§»é™¤å†—ä½™è¯æ±‡å’ŒçŸ­è¯­
        compressed = paragraph
        
        # ç§»é™¤å¸¸è§çš„å¡«å……è¯
        filler_words = ['çš„è¯', 'å…¶å®', 'åŸºæœ¬ä¸Š', 'äº‹å®ä¸Š', 'ä¸€èˆ¬æ¥è¯´', 'æ€»çš„æ¥è¯´']
        for word in filler_words:
            compressed = compressed.replace(word, '')
        
        # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œå–å‰åŠéƒ¨åˆ†
        if len(compressed) > 200:
            compressed = compressed[:200] + "..."
        
        return compressed.strip()


class MultimodalContentProcessor(ContentProcessor):
    """å¤šåª’ä½“å†…å®¹å¤„ç†å™¨"""
    
    def process_with_images(
        self, 
        url: str, 
        content: str, 
        images: List[str],
        available_tokens: int = None
    ) -> Tuple[str, List[str]]:
        """å¤„ç†åŒ…å«å›¾ç‰‡çš„å†…å®¹"""
        
        # å¤„ç†æ–‡æœ¬å†…å®¹
        text_content = self.process_crawled_content(url, content, available_tokens)
        
        # æ™ºèƒ½é€‰æ‹©å›¾ç‰‡
        selected_images = self._select_important_images(images, max_count=3)
        
        return text_content, selected_images
    
    def _select_important_images(self, images: List[str], max_count: int) -> List[str]:
        """æ™ºèƒ½é€‰æ‹©é‡è¦å›¾ç‰‡"""
        if len(images) <= max_count:
            return images
        
        # ç®€å•ç­–ç•¥: ä¼˜å…ˆé€‰æ‹©å¯èƒ½åŒ…å«é‡è¦ä¿¡æ¯çš„å›¾ç‰‡
        scored_images = []
        for img in images:
            score = self._score_image_importance(img)
            scored_images.append((img, score))
        
        # æŒ‰å¾—åˆ†æ’åºå¹¶é€‰æ‹©top N
        scored_images.sort(key=lambda x: x[1], reverse=True)
        return [img for img, _ in scored_images[:max_count]]
    
    def _score_image_importance(self, image_url: str) -> float:
        """ä¸ºå›¾ç‰‡é‡è¦æ€§è¯„åˆ†"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # åŒ…å«chart, graph, diagramç­‰å…³é”®è¯çš„å›¾ç‰‡æ›´é‡è¦
        important_keywords = ['chart', 'graph', 'diagram', 'data', 'statistics']
        for keyword in important_keywords:
            if keyword in image_url.lower():
                score += 0.2
        
        # é¿å…å¹¿å‘Šå’Œè£…é¥°æ€§å›¾ç‰‡
        ad_keywords = ['ads', 'banner', 'logo', 'avatar']
        for keyword in ad_keywords:
            if keyword in image_url.lower():
                score -= 0.3
        
        return max(0.0, min(1.0, score))


# å…¨å±€å®ä¾‹
content_processor = ContentProcessor()
multimodal_processor = MultimodalContentProcessor()